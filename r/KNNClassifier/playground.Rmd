---
title: "KNN Classifier Playground"
output: html_document
date: "2024-04-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Simulate Data for Exploration

For exploration of the the usage of the `KNNClassifier` functionality, I simulate
a data set that is easy to visualize, but that contains irregularities that would usually
necessitate the use of a flexible classifier like the KNN algorithm. The response
variable contains three classes, and the distributions for the two features measured
on each observation within each group is a multivariate normal.

```{r}
library(mvtnorm)

# set seed for reproducibility
set.seed(314)

# samples per group
n_samples <- 500
# add some weird behavior within a group
n_diff <- 200

# define covariance matrices; one group is much more variable and correlated
sig2 <- 1
S <- list(
  s1 = sig2*matrix(c(1, -0.5, -0.5, 1), ncol = 2),
  s2 = sig2*matrix(c(1, 0.2, 0.2, 1), ncol = 2),
  s3 = sig2*matrix(c(3, -0.2, -0.2, 3), ncol = 2)
)

# randomly draw from different MVN distributions for each group
X1 <- rbind(
  rmvnorm(n_samples - n_diff, mean = c(1, 2), sigma = S$s1),
  rmvnorm(n_diff, mean = c(-4, -5), sigma = S$s1)
)
X2 <- rbind(
  rmvnorm(n_samples - n_diff, mean = c(0, 1), sigma = S$s2),
  rmvnorm(n_diff, mean = c(-2, -3), sigma = S$s2)
)
X3 <- rbind(
  rmvnorm(n_samples - n_diff, mean = c(-5, 0), sigma = S$s3),
  rmvnorm(n_diff, mean = c(2, -4), sigma = S$s3)
)


# combine data
X <- rbind(X1, X2, X3)
y <- c(rep("steelblue", n_samples), rep("orange2", n_samples), rep("forestgreen", n_samples))

dat <- data.frame(X, y)

```

The scatter plot below demonstrates the nature the of the simulated data.

```{r}
library(scales)
plot(X, col = alpha(dat$y, 0.5), pch = 19, xlab = "Feature 1", ylab = "Feature 2",
     main = "Simulated Class Membership Data", cex = 1.25)
legend('bottomright', bty = 'n', legend = c("Class 1", "Class 2", "Class 3"),
       col = alpha(c("steelblue", "orange2", "forestgreen"), 0.5), pch = 19)
```

## Data Preparation

For this example I convert the response variable to a factor, since that is how I
programmed the functions to work, and this is common for treatment of categorical
outcomes in other libraries. I split the full data set `dat` into `dat_train` and 
`dat_test` with a 80/20 split, respectively. I center and scale the predictive
features before feeding the data into the classifier. 

It is important to note that normalization of the all data must be done as if the
training data were the only observations available. Thus I define a "scaler" that
operates with the sample statistics only from `dat_train`.

```{r}
dat$y <- as.factor(dat$y)

train_index <- sample(nrow(dat), size = 0.8*nrow(dat), replace = FALSE)

dat_train <- dat[train_index,]

# output a function that has training data stats in scope
Scaler <- function(x_train) {
  m_center <- colMeans(x_train)
  m_scale <- apply(x_train, 2, sd)
  function(X) {
    X_scaled <- X
    for (i in 1:ncol(X)) {
      X_scaled[i] <- (X[i] - m_center[i]) / m_scale[i] 
    }
    X_scaled
  }
}

scaler <- Scaler(dat_train[, 1:2])

dat_test <- dat[-train_index,]

# scaled feature matrices
X_test_scaled <- scaler(dat_test[,1:2])
X_train_scaled <- scaler(dat_train[,1:2])

# split response vectors
y_train <- dat_train$y
y_test <- dat_test$y
```

## Model Fitting

Before we go ahead and find a more "optimal" value for `k`, lets just take a look
at the functionality defined in `KNNClassifier.R`.

```{r}
source("KNNClassifier.R")
k <- 1
knn_classifier <- KNNClassifier(k)

knn_fit <- knn_classifier(X = X_train_scaled, y = y_train)

y_hat <- knn_fit(X_test_scaled)
test_accuracy <- mean(y_hat == y_test)
print(paste0("The model's test accuracy for k=", k, " was ", round(test_accuracy*100, 2), "%."))
```

## Model Performance Assessment

Now let's iterate through different values of `k` to see if any improvements can 
be made, and use the average of each class's *F1* score as our performance metric.

```{r}
# F1 scores for all classes
f1_scores <- function(y_pred) {
  classes <- as.character(unique(dat$y))
  f1 <- numeric(length(classes))
  
  # calculate f1 for each output class
  for (i in 1:length(classes)) {
    z <- classes[i]
    
    TP <- sum(y_pred[y_test == z] == z)
    FN <- sum(y_pred[y_test == z] != z)
    FP <- sum(y_pred[y_test != z] == z)
  
    recall <- TP / (TP + FN)
    precision <- TP / (TP + FP)
  
    f1[i] <- (2 * precision * recall) / (precision + recall)
  }
  f1
}

k_values <- 1:100
f1_score_means <- numeric(length(k_values))

# for each unique value of k
for (i in k_values) {
  # define the classifier
  knn_classifier <- KNNClassifier(i)
  
  # fit to the (scaled) training data
  knn_fit <- knn_classifier(X = X_train_scaled, y = y_train)

  # predict on out-of-sample (scaled) data
  y_hat <- knn_fit(X_test_scaled)
  
  # calculate average f1 score
  f1_score_means[i] <- mean(f1_scores(y_hat))
}

```

The plot below illustrates how the model's predictive performance changes with different
values of `k`.

```{r}
plot(k_values, f1_score_means, main = "Average F1 by K", xlab = "k", 
     ylab = "Average F1", type = "l")
```

It appears that there are diminishing returns in model performance after `k` is 
approximately in range of `r which.max(f1_score_means) - 2` to `r which.max(f1_score_means) + 2`. Values of `k > 20` experiences a performance plateau and eventual drop-off. I would recommend a value 
of `k=10` based on the evaluation above.